{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Byte Pair Encoding (BPE) - Uzman**\n",
        "\n",
        "Ã–nceki Ã§alÄ±ÅŸmamÄ±zda BPE algoritmasÄ±nÄ±n Ã§ekirdeÄŸini (istatistik al -> birleÅŸtir) sÄ±fÄ±rdan inÅŸa ettik. Ancak modern LLM'ler (GPT serisi) bu iÅŸlemi ham metin Ã¼zerinde doÄŸrudan yapmazlar; verimlilik ve baÄŸlam yÃ¶netimi iÃ§in ek katmanlar kullanÄ±rlar.\n",
        "\n",
        "Bu notebook'ta, gerÃ§ek bir LLM tokenizer'Ä±nÄ±n sahip olduÄŸu geliÅŸmiÅŸ Ã¶zellikleri ve endÃ¼stri standartlarÄ±nÄ± inceleyeceÄŸiz:\n",
        "\n",
        "1.  **Regex (DÃ¼zenli Ä°fadeler):** Metni BPE'ye sokmadan Ã¶nce mantÄ±ksal parÃ§alara (kelime, noktalama, sayÄ±) ayÄ±rma stratejisi.\n",
        "2.  **Special Tokens:** `<|endoftext|>` gibi modelin kontrol sinyallerini yÃ¶netme.\n",
        "3.  **Tiktoken:** OpenAI'Ä±n resmi kÃ¼tÃ¼phanesi ve GPT-2'nin gerÃ§ek kelime daÄŸarcÄ±ÄŸÄ± dosyalarÄ±nÄ±n analizi.\n",
        "\n",
        "> ğŸ”— **Kaynaklar ve Referanslar:**\n",
        "> * **GÃ¶rselleÅŸtirme AracÄ±:** Metinlerin modeller tarafÄ±ndan nasÄ±l parÃ§alandÄ±ÄŸÄ±nÄ± canlÄ± gÃ¶rmek iÃ§in harika bir araÃ§: [Tiktokenizer App](https://tiktokenizer.vercel.app/)\n",
        "> * **Orijinal Kaynak Kod:** OpenAI GPT-2 modelinin temelindeki resmi Python uygulamasÄ± ve Regex desenleri: [GitHub: GPT-2](https://github.com/openai/gpt-2/blob/master/src/encoder.py)\n",
        "> * **Derinlemesine Ä°nceleme:** Ã–zel tokenlarÄ±n (Special Tokens) mantÄ±ÄŸÄ± ve kullanÄ±mÄ± Ã¼zerine topluluk tartÄ±ÅŸmasÄ±: [Reddit: ChatGPT Special Tokens](https://www.reddit.com/r/ChatGPT/comments/14afi5g/chatgpt_special_tokens/)"
      ],
      "metadata": {
        "id": "77fFuVFQIHCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/omertarikyilmaz/byte-pair-encoding/blob/main/assets/cahit_arf.jpg?raw=true\" alt=\"Cahit Arf\" width=\"800\">\n",
        "</p>"
      ],
      "metadata": {
        "id": "L_lYTprtQ0kc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fle6ubmHOby"
      },
      "outputs": [],
      "source": [
        "# @title Veri Seti Cahit Arf (Ã‡ift TÄ±klayarak AÃ§abilirsin)\n",
        "dataset = \"\"\"Ord. Prof. Dr. Cahit Arf (1910 - 1997), cebir ve sayÄ±lar teorisi alanÄ±ndaki Ã§alÄ±ÅŸmalarÄ±yla dÃ¼nya Ã§apÄ±nda tanÄ±nan, \"Arf Sabiti\", \"Arf HalkalarÄ±\" ve \"Arf KapanÄ±ÅŸlarÄ±\" gibi matematiksel terimleri bilim dÃ¼nyasÄ±na kazandÄ±ran TÃ¼rk matematikÃ§idir. 1910 yÄ±lÄ±nda Selanik'te doÄŸan Arf, ilkokulu o yÄ±llarda sultani adÄ± verilen liselerin ilk kÄ±smÄ±nda okumuÅŸ, daha sonra Paris'te St. Louis Lisesi'nde eÄŸitimini tamamlamÄ±ÅŸtÄ±r. YÃ¼ksekÃ¶ÄŸrenimini Fransa'da Ecole Normale Superieure'de 1932 yÄ±lÄ±nda tamamlayarak TÃ¼rkiye'ye dÃ¶nmÃ¼ÅŸtÃ¼r. Bir sÃ¼re Galatasaray Lisesi'nde matematik Ã¶ÄŸretmenliÄŸi yaptÄ±ktan sonra, Ä°stanbul Ãœniversitesi Fen FakÃ¼ltesi'nde doÃ§ent adayÄ± olarak Ã§alÄ±ÅŸmaya baÅŸlamÄ±ÅŸtÄ±r. DoktorasÄ±nÄ± yapmak Ã¼zere 1937 yÄ±lÄ±nda Almanya'ya, GÃ¶ttingen Ãœniversitesi'ne gitmiÅŸ ve burada Ã¼nlÃ¼ matematikÃ§i Helmut Hasse ile Ã§alÄ±ÅŸmÄ±ÅŸtÄ±r. Hasse'nin \"Bu konuda doktora yapan Ã¶ÄŸrencilerim 1.5 - 2 yÄ±lda bitiriyor\" dediÄŸi zorlu doktora Ã§alÄ±ÅŸmasÄ±nÄ± Cahit Arf, 1938 yÄ±lÄ±nda tamamlayarak bÃ¼yÃ¼k bir baÅŸarÄ±ya imza atmÄ±ÅŸtÄ±r. Bu Ã§alÄ±ÅŸma sonucunda, cisimlerin kuadratik formlarÄ±nÄ±n sÄ±nÄ±flandÄ±rÄ±lmasÄ±nda ortaya Ã§Ä±kan ve bugÃ¼n literatÃ¼rde \"Arf DeÄŸiÅŸmezi\" (Arf Invariant) olarak bilinen deÄŸiÅŸmezi bulmuÅŸtur. Bu keÅŸif, karakteristiÄŸi 2 olan cisimler Ã¼zerindeki kuadratik formlar teorisine kÃ¶klÃ¼ bir katkÄ± saÄŸlamÄ±ÅŸtÄ±r. AyrÄ±ca \"Hasse-Arf Teoremi\" ile matematikte kalÄ±cÄ± bir iz bÄ±rakmÄ±ÅŸtÄ±r. TÃ¼rkiye'ye dÃ¶ndÃ¼ÄŸÃ¼nde Ä°stanbul Ãœniversitesi'nde Ã§alÄ±ÅŸmaya devam etmiÅŸ, 1943'te profesÃ¶r, 1955'te ordinaryÃ¼s profesÃ¶r unvanÄ±nÄ± almÄ±ÅŸtÄ±r. 1962 yÄ±lÄ±nda dÃ¶nemin CumhurbaÅŸkanÄ± Cemal GÃ¼rsel'in atamasÄ±yla TÃœBÄ°TAK'Ä±n (TÃ¼rkiye Bilimsel ve Teknolojik AraÅŸtÄ±rma Kurumu) kuruluÅŸ Ã§alÄ±ÅŸmalarÄ±nda kurucu Ã¼ye olarak yer almÄ±ÅŸ ve uzun yÄ±llar Bilim Kurulu BaÅŸkanlÄ±ÄŸÄ± yapmÄ±ÅŸtÄ±r. Cahit Arf, matematiÄŸi sadece bir meslek deÄŸil, bir yaÅŸam biÃ§imi ve sanat olarak gÃ¶rmÃ¼ÅŸtÃ¼r. Onun ÅŸu sÃ¶zÃ¼ meÅŸhurdur: \"Matematik esas olarak sabÄ±r olayÄ±dÄ±r. Belleyerek deÄŸil, keÅŸfederek anlamak gerekir.\" Robert Kolej'de, ODTÃœ'de (Orta DoÄŸu Teknik Ãœniversitesi) ve yurt dÄ±ÅŸÄ±nda Institute for Advanced Study (Princeton) ile California Ãœniversitesi'nde (Berkeley) misafir Ã¶ÄŸretim Ã¼yesi olarak bulunmuÅŸtur. 1948'de Ä°nÃ¶nÃ¼ ArmaÄŸanÄ±'nÄ±, 1974'te TÃœBÄ°TAK Bilim Ã–dÃ¼lÃ¼'nÃ¼ kazanmÄ±ÅŸtÄ±r. 1980 yÄ±lÄ±nda Ä°TÃœ ve Karadeniz Teknik Ãœniversitesi, 1981 yÄ±lÄ±nda ise ODTÃœ kendisine onursal doktora unvanÄ± vermiÅŸtir. BugÃ¼n kullandÄ±ÄŸÄ±mÄ±z 10 TÃ¼rk LirasÄ± banknotlarÄ±nÄ±n arka yÃ¼zÃ¼nde Cahit Arf'Ä±n portresi ve \"Arf DeÄŸiÅŸmezi\" formÃ¼lÃ¼nden bir kesit yer almaktadÄ±r. Cahit Arf, 26 AralÄ±k 1997'de Ä°stanbul'da geÃ§irdiÄŸi kalp rahatsÄ±zlÄ±ÄŸÄ± sonucu hayatÄ±nÄ± kaybetmiÅŸ, ancak geride bÄ±raktÄ±ÄŸÄ± teoremlerle Ã¶lÃ¼msÃ¼zleÅŸmiÅŸtir.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Regex ile Zorunlu BÃ¶lme (GPT-2 Deseni)**\n",
        "\n",
        "Standart BPE algoritmasÄ±nÄ±n en bÃ¼yÃ¼k riski, kelimeleri noktalama iÅŸaretleriyle birleÅŸtirmesidir (Ã¶rneÄŸin \"okul\" ve \"okul.\" farklÄ± token olur). GPT-2 bunu Ã¶nlemek iÃ§in metni BPE iÅŸleminden Ã¶nce Ã¶zel bir **Regex (DÃ¼zenli Ä°fade)** deseniyle parÃ§alar.\n",
        "\n",
        "GÃ¶rselde yer alan ve aÅŸaÄŸÄ±da uygulayacaÄŸÄ±mÄ±z bu desen, metni ÅŸu kategorilere ayÄ±rÄ±r:\n",
        "* KÄ±saltmalar (Ä°ngilizce `'s`, `'t` vb.)\n",
        "* Kelime Karakterleri (Harfler)\n",
        "* SayÄ±lar\n",
        "* Kelimelerden ayrÄ± durmasÄ± gereken noktalama iÅŸaretleri\n",
        "* BoÅŸluklar\n",
        "\n",
        "Bu sayede \"Cahit Arf (1910)\" ifadesi `['Cahit', 'Arf', '(', '1910', ')']` olarak temizce ayrÄ±lÄ±r."
      ],
      "metadata": {
        "id": "6lWntstTOAtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import regex as re\n",
        "\n",
        "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
        "\n",
        "print(re.findall(gpt2pat, \"Cahit Arf (1910)\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elOSU2FKN934",
        "outputId": "c683e3a5-40fc-4f71-f47d-60299a6264ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Cahit', ' Arf', ' (', '1910', ')']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = re.findall(gpt2pat, dataset)\n",
        "\n",
        "print(f\"Dataset Karakter SayÄ±sÄ±: {len(dataset)}\")\n",
        "print(f\"Regex ParÃ§a SayÄ±sÄ±:      {len(tokens)}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(tokens[10:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmLgkk5qPPEQ",
        "outputId": "5d5109fb-e67b-4f35-a6dd-dcc9d249bfcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Karakter SayÄ±sÄ±: 2583\n",
            "Regex ParÃ§a SayÄ±sÄ±:      448\n",
            "--------------------------------------------------\n",
            "[' -', ' 1997', '),', ' cebir', ' ve', ' sayÄ±lar', ' teorisi', ' alanÄ±ndaki', ' Ã§alÄ±ÅŸmalarÄ±yla', ' dÃ¼nya', ' Ã§apÄ±nda', ' tanÄ±nan', ',', ' \"', 'Arf', ' Sabiti', '\",', ' \"', 'Arf', ' HalkalarÄ±', '\"', ' ve', ' \"', 'Arf', ' KapanÄ±ÅŸlarÄ±', '\"', ' gibi', ' matematiksel', ' terimleri', ' bilim', ' dÃ¼nyasÄ±na', ' kazandÄ±ran', ' TÃ¼rk', ' matematikÃ§idir', '.', ' 1910', ' yÄ±lÄ±nda', ' Selanik', \"'t\", 'e']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stats(ids):\n",
        "    counts = {}\n",
        "    for pair in zip(ids, ids[1:]):\n",
        "        counts[pair] = counts.get(pair, 0) + 1\n",
        "    return counts\n",
        "\n",
        "def merge(ids, pair, idx):\n",
        "  newids = []\n",
        "  i = 0\n",
        "  while i < len(ids):\n",
        "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
        "      newids.append(idx)\n",
        "      i += 2\n",
        "    else:\n",
        "      newids.append(ids[i])\n",
        "      i += 1\n",
        "  return newids"
      ],
      "metadata": {
        "id": "R83jh7NcLq5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 276\n",
        "num_merges = vocab_size - 256\n",
        "ids = list(dataset.encode(\"utf-8\"))\n",
        "\n",
        "merges = {}\n",
        "for i in range(num_merges):\n",
        "  stats = get_stats(ids)\n",
        "  pair = max(stats, key=stats.get)\n",
        "  idx = 256 + i\n",
        "\n",
        "  ids = merge(ids, pair, idx)\n",
        "  merges[pair] = idx"
      ],
      "metadata": {
        "id": "NRDpNyMBL1zD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
        "for (p0, p1), idx in merges.items():\n",
        "    vocab[idx] = vocab[p0] + vocab[p1]"
      ],
      "metadata": {
        "id": "eW-2UhGSMFSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(ids):\n",
        "  tokens = b\"\".join(vocab[idx] for idx in ids)\n",
        "  text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
        "  return text\n",
        "\n",
        "def encode(text):\n",
        "  tokens = list(text.encode(\"utf-8\"))\n",
        "  while len(tokens) >= 2:\n",
        "    stats = get_stats(tokens)\n",
        "    pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
        "    if pair not in merges:\n",
        "      break\n",
        "\n",
        "    idx = merges[pair]\n",
        "    tokens = merge(tokens, pair, idx)\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "FrjFey0eMGIq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}